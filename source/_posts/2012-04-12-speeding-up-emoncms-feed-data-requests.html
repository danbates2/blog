---
layout: post
title: Speeding up emoncms feed data requests
date: '2012-04-12T10:34:00.003-07:00'
author: Trystan Lea
categories:
- emoncms
modified_time: '2012-05-25T03:10:51.801-07:00'
thumbnail: http://2.bp.blogspot.com/-mLgSqKaw8Ng/T4cQ9I1n-XI/AAAAAAAACZw/1zFfpAL2ZPo/s72-c/queryspeeddata.png
blogger_id: tag:blogger.com,1999:blog-2472065242652647619.post-5700542033666694823
blogger_orig_url: http://blog.openenergymonitor.org/2012/04/speeding-up-emoncms-feed-data-requests.html
---

<br />If you log 5 second data you quickly&nbsp;amass&nbsp;a huge number of&nbsp;datapoints&nbsp;(about 6 million a year). If you then tried to visualise a years data by loading all 6 million datapoints you would be waiting a long time. The most we would want to load is the same number of datapoints as the pixel width of the visualisation, lets say around 1000 datapoints. So we need some way of picking out of our 6 million datapoints 1000 datapoints at&nbsp;equal&nbsp;interval.<br /><br />When I first wrote emoncms I searched&nbsp;for mysql queries that could pick out table rows at given intervals and came across the following query&nbsp;that does this:<br /><br /><pre>SELECT * FROM (SELECT @row := @row +1 AS rownum, time,data FROM ( SELECT @row :=0) r, $feedname) ranked WHERE (rownum % $resolution = 1) AND (time&gt;'$start' AND time&lt;'$end') order by time Desc</pre><br />It seem to work really well. Fast forward 5 months my feed tables are approaching 2.5 million rows &nbsp;and I'm starting to notice query times getting really quite long. After a bit of searching again, I came across a suggestion to a similar problem suggesting the use of an index. So I tried adding an index and creating a php for-loop to request a single row at given intervals:<br /><br /><pre>$range = $end - $start;<br />$interval = $range / 1000;<br /><br />for ($i=0; $i&lt;1000; $i++)<br />{<br />&nbsp; $qtime = $start + $i * $interval;<br />&nbsp; $result = db_query("SELECT * FROM $feedname WHERE `time` &gt;$qtime LIMIT 1");</pre><pre>&nbsp; if($result){<br />&nbsp; &nbsp; $row = db_fetch_array($result);<br />&nbsp; &nbsp; $data[] = array($row['time'] * 1000, $row['data']); &nbsp; &nbsp; <br />&nbsp; }</pre>}<br /><br />The following table shows the typical times for requesting data from both the indexed and non indexed tables and the different methods. At the same time I also tested&nbsp;whether&nbsp;request times where affected by the data type that time is stored as: mysql datetime or a unix timestamp stored as an unsigned int.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-mLgSqKaw8Ng/T4cQ9I1n-XI/AAAAAAAACZw/1zFfpAL2ZPo/s1600/queryspeeddata.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="113" src="http://2.bp.blogspot.com/-mLgSqKaw8Ng/T4cQ9I1n-XI/AAAAAAAACZw/1zFfpAL2ZPo/s640/queryspeeddata.png" width="640" /></a></div><br /><br />So it looks like the optimum configuration is primarily the addition of an index and use of the php data request method and then to reduce disk space use, switching to unix timestamp. The next step is to create a script to automate table conversion from datetime unindexed to timestamp indexed.<br /><div><br /></div><br /><br />